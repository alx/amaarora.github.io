# ConvNext: A ConvNet for the 2020s

1. TOC 
{:toc}


## Objective
What's my goal as I write this blog post? I want this blog post to be the only reference that you'll need to understand everything about the ConvNext architecture including its implementation in [PyTorch](https://pytorch.org/)! 

## Introduction
As part of this blog post, I will be going through the [ConvNext](https://arxiv.org/abs/2201.03545) architecture and will be sharing all inner workings along with PyTorch code from the [official implementation](https://github.com/facebookresearch/ConvNeXt).

But before we begin, why ConvNext? Why should you learn about this architecture? In my humble opinion, if there is any architecture in 2022 so far that is as simple as [ResNets](https://arxiv.org/abs/1512.03385?context=cs) to understand and can compete with ViTs in terms of performance, it is **ConvNexts**! 

In the paper, the researchers start with a standard ResNet architecture, and gradually "modernize" it to towards a Visual Transformer, and more specifically - [Swin Transformer](https://amaarora.github.io/2022/07/04/swintransformerv1.html). 

In this blog post, we will follow the same pattern, and start with a standard ResNet `BottleNeck` block and gradually "modernize" it to ConvNext `Block`. 

> Note: All text directly copied from the [ConvNext paper](https://arxiv.org/abs/2201.03545) will be presented in *Italics*.

## Prerequisite
As part of this blog post, I am going to assume that you know about the following architectures: 
1. ViT (Vision Transformer)
2. Swin Transformer
3. ResNet Architcture
4. ResNext Architecture

## Abstract
From the abstract of the paper: 

*In this work, we reexamine the design spaces and test the limits of what a pure ConvNet can achieve. We gradually “modernize” a standard ResNet toward the design of a vision Transformer, and discover several key components that contribute to the performance difference along the way. The outcome of this exploration is a family of pure ConvNet models dubbed ConvNeXt. Constructed entirely from standard ConvNet modules, ConvNeXts compete favorably with Transformers in terms of accuracy and scalability, achieving 87.8% ImageNet top-1 accuracy and outperforming Swin Transformers on COCO detection and ADE20K segmentation, while maintaining the simplicity and efficiency of standard ConvNets.*

## Context
In this section, I will share some background that might help the reader in understanding how the ConvNexts came to life. Essentially the researchers borrowed all the great ideas from some of the historic papers in the field of Computer Vision in the past. 

For example, the [ResNet architecture](https://arxiv.org/abs/1512.03385?context=cs) in 2015, the [ResNext](https://arxiv.org/abs/1611.05431?context=cs) and [Squeeze & Excitation](https://arxiv.org/abs/1709.01507) architectures in 2017, [MobileNetV2](https://arxiv.org/abs/1801.04381) architecture in 2018, the [Vision Transformer](https://arxiv.org/abs/2010.11929) in 2020 or the [Swin Transformer](https://arxiv.org/abs/2103.14030) in 2021 - this paper borrows all the good ideas from each of these groundbreaking research papers in the past and puts them all together to introduce the powerful ConvNext architecture.

I've previously presented a talk at Queensland AI Hub about [What's new in Computer Vision](https://www.youtube.com/watch?v=IYg46wNyDgo). While this talk became outdated very quickly, thanks to the rapid ongoing research in the field, as part of the talk I've shared some of the major papers in Computer Vision. 

I've written about almost all of these ideas/papers in my previous blog posts and will be referencing them as and when required. 

## Swin Transformer
Why do Swin Transformers get a separate section as part of this blog post? From the Introduction section of the ConvNext paper: 

*Without the ConvNet inductive biases, a vanilla ViT model faces many challenges in being adopted as a generic vision backbone. The biggest challenge is ViT’s global attention design, which has a quadratic complexity with respect to the input size. This might be acceptable for ImageNet classification, but quickly becomes intractable with higher-resolution inputs.*

*Hierarchical Transformers employ a hybrid approach to bridge this gap. For example, the “sliding window” strategy (e.g. attention within local windows) was reintroduced to Transformers, allowing them to behave more similarly to ConvNets. **Swin Transformer is a milestone work in this direction, demonstrating for the first time that Transformers can be adopted as a generic vision backbone and achieve state-of-the-art performance across a range of computer vision tasks beyond image classification.***

Most of the ideas in this ConvNext paper have been borrowed directly from the Swin Transformer, such as:

1. Architecture design
2. Inverted bottleneck design
3. Patchify stem implemented with a "4x4" non-overlapping convolution
4. Use of larger kernel sizes 

**Need a refresher on Swin Transformer?** Refer to my previous blog post ➡️ [Swin Transformer: Hierarchical Vision Transformer using Shifted Windows](https://amaarora.github.io/2022/07/04/swintransformerv1.html)

## ResNet Block 
A standard ResNet `BottleNeck` block can be implemented as below: 

```python 
class Block(nn.Module):
    def __init__(self, in_ch, out_ch, hidden_dim, drop_path=0., layer_scale_init_value=1e-6):
        super().__init__()
        self.conv1 = nn.Conv2d(in_ch, out_ch//4, kernel_size=1, bias=False) 
        self.bn1 = nn.BatchNorm2d(out_ch//4)
        self.conv2 = nn.Conv2d(out_ch//4, out_ch//4, kernel_size=3, bias=False, padding=1) 
        self.bn2 = nn.BatchNorm2d(out_ch//4)
        self.act = nn.ReLU()
        self.conv3 = nn.Conv2d(out_ch//4, out_ch, kernel_size=1, bias=False) 
        self.bn3 = nn.BatchNorm2d(out_ch)

    def forward(self, x):
        input = x
        x = self.conv1(x)
        x = self.bn1(x)
        x = self.conv2(x)
        x = self.bn2(x)
        x = self.conv3(x)
        x = self.bn3(x)
        return input + x
```