# (TIMM SERIES 1) AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE


## Preface
As you might have guessed from the title already, today, we will be looking into Vision Transformers and implement the architecture in PyTorch using code from one of my favourite libraries - [timm](https://github.com/rwightman/pytorch-image-models) created by [Ross Wightman](https://twitter.com/wightmanr).

This is the first blog in the new [TIMM SERIES](https://twitter.com/amaarora/status/1350410770052313088). Over time, we will be looking at the various architectures inside **timm** alongside the research papers to understand the implementation. All code in this blog post and every future **TIMM SERIES** post will be referencing the code from [timm](https://github.com/rwightman/pytorch-image-models) created by [Ross Wightman](https://twitter.com/wightmanr). The code from this blog post has been directly copied from [here](https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vision_transformer.py).

## Prerequisite
In this blog post, I assume that the reader knows about the [Transformer Architecture](https://arxiv.org/abs/1706.03762). It will not be introduced as part of this post but rather we will be looking at how the authors of [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929) from Google Brain applied the Transformer Architecture to computer vision.

Here are some of my personal favourite resources on Transformers:
1. [The illustrated Transformer](http://jalammar.github.io/illustrated-transformer/) by Jay Alammar
2. [The Annotated Transformer](https://nlp.seas.harvard.edu/2018/04/03/attention.html) by Harvard NLP 
3. [Introduction to the Transformer](https://www.youtube.com/watch?v=AFkGPmU16QA&list=PLtmWHNX-gukKocXQOkQjuVxglSDYWsSh9&index=18&t=0s) by Rachel Thomas and Jeremy Howard

Though a biased recommendation, I would also like to recommend the reader to my previous post [The Annotated GPT-2](https://amaarora.github.io/2020/02/18/annotatedGPT2.html).

## Introduction
At the time of release, the [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929) has received quite a bit of "attention" from the community. This was the first paper to get some astonishing results on the ImageNet dataset using the Transformer architecture. While there have been attempts made in the past to apply Transformers in the context of image processing ([1](https://arxiv.org/abs/1802.05751), [2](https://arxiv.org/abs/1906.05909), [3](https://arxiv.org/abs/2003.07853)), the Vision Transformer was one of the first to apply Transformers to full-sizes images. 

**NOTE:** I will be using Vision Transforms, ViT interchangeably throughout this blog post and both refer to [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929) paper.

The key contributions from this paper were not in terms of a new architecture, but rather the application of an existing architecture (Transformers), to a new field. It is the **training method** and the **dataset used to pretrain the network**, that were key for ViT to get excellent results compared to SOTA (State of the Art) on ImageNet.

So, there isn't a lot of new things to introduce in this post, but rather how to use the existing knowledge and apply the Transformer architecutre to images. Thus, if the reader knows about Transformers, this blog post and the research paper itself should be a fairly simple read.

## Method
So, let's get right into the method and understand all the magic in the training pipeline. 

![](/images/ViT.png "fig-1 Model Overview")

From the paper: 
> Inspired by the Transformer scaling successes in NLP, we experiment with applying a standard Transformer directly to images, with the fewest possible modifications. To do so, we split an image into patches and provide the sequence of linear embeddings of these patches as an input to a Transformer. Image patches are treated the same way as tokens (words) in an NLP application. We train the model on image classification in supervised fashion.

> When trained on mid-sized datasets such as ImageNet, such models yield modest accuracies of a few percentage points below ResNets of comparable size. This seemingly discouraging outcome may be expected: Transformers lack some of the inductive biases inherent to CNNs, such as translation equivariance and locality, and therefore do not generalize well when trained on insufficient amounts of data.

> However, the picture changes if the models are trained on larger datasets (14M-300M images). We find that large scale training trumps inductive bias. Our Vision Transformer (ViT) attains excellent results when pre-trained at sufficient scale and transferred to tasks with fewer datapoints. When pre-trained on the public ImageNet-21k dataset or the in-house JFT-300M dataset, ViT approaches or beats state of the art on multiple image recognition benchmarks. In particular, the best model reaches the accuracy of 88.55% on ImageNet, 90.72% on ImageNet-ReaL, 94.55% on CIFAR-100, and 77.63% on the VTAB suite of 19 tasks.

So, as can be read from above, the key contributions from the paper are: 
1. The idea to split an image into patches before feeding to the `Transformer Encoder`
2. Pretraining on large amounts of data 

Although, the pretraining dataset used in this case was `JFT-300M` which consists of 300 million images and it takes around 2.5K TPU V3 core-days to pretrain such a network to get competent accuracies by just using a Transformer compared to CNNs. Thus, this is reason enough that Google Brain was the first to come up with such great results and accuracy on the ImageNet dataset.

At this point, I have already presented the overall idea of the paper. 

Let's now look into some of the minor details. 

## Vision Transformer
An overview of the model is shown in `fig-1`. In this section we will be looking at all of the minor details inside the Vision Transformer architecture. 

**Step-1: How to get the image patches?**
To get the patches from 2D images as input, the authors reshapred the image into a sequence of flattened 2D patches. Let's consider the input image has `H` height, `W` width and `C` number of channels. Thus the dimensions are **H x W x C**. The authors reshaped this image into **N x (P<sup>2</sup>) x C)** where `N` is the number of patches, `P x P` is the resolution of each image batch and `C` remains the same.

> As an example, let's consider an input image of size `224 x 224 x 3`. We could reshape it into `196` patches of size `16 x 16 x 3`.

**Step-2: What's next? What we do with the image patches?**
We linearly project the patches (which can be thought of as `tokens` in NLP) to a dimension `D` using a linear projection.

> Confusing? If we flatten the `16 * 16 * 3` sized patch, we get a vector of length `768`. This linear projection could as an example be a Linear layer followed by a `ReLU` or a 1-D Conv to get a representation vector of size `768` itself. Thus we now have `196` linear projections of these flattened patches as the output.

**Step-3: Some extra preprocessing before sending the input to the `Transformer Encoder`**
As we know from `GPT-2` or `BERT`, there is a `[cls]` token at the beginning of the sequence whose state at the output of the `Transformer Encoder` serves as the sequence representation in NLP, similarly the authors prepend this token to the flat 1-D patch from `step-2` whose state at the output of the `Transformer Encoder` serves as the image representation. 

Also, as you know, the `Transformer Architecture` doesn't have an inherent knowledge about the positional information, thus **Position Embeddings** are also added to the patch embeddings to retain the positional information. The authors use standard 1D positional embeddings similar to NLP based tasks.

At this stage, the inputs are ready to be passed to the `Transformer Encoder`.

**Step-4: MLP Head**
We get the outputs from the `Transformer Encoder`, extract the representation of the `cls` token and use a MLP head to get class predictions.

That is really it. There are also some extra minor details regarding `GELU` non-linearity being used in MLP and that the MLP consists of two layers, which in the big scheme of things really are just minor details.

