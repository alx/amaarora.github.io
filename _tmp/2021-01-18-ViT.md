# (TIMM SERIES 1) AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE


## Preface
As you might have guessed from the title already, today, we will be looking into Vision Transformers and implement the architecture in PyTorch using code from one of my favourite libraries - [timm](https://github.com/rwightman/pytorch-image-models) created by [Ross Wightman](https://twitter.com/wightmanr).

This is the first blog in the new [TIMM SERIES](https://twitter.com/amaarora/status/1350410770052313088). Over time, we will be looking at the various architectures inside **timm** alongside the research papers to understand the implementation. All code in this blog post and every future **TIMM SERIES** post will be referencing the code from [timm](https://github.com/rwightman/pytorch-image-models) created by [Ross Wightman](https://twitter.com/wightmanr). The code from this blog post has been directly copied from [here](https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vision_transformer.py).

## Prerequisite
In this blog post, I assume that the reader knows about the [Transformer Architecture](https://arxiv.org/abs/1706.03762). It will not be introduced as part of this post but rather we will be looking at how the authors of [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929) from Google Brain applied the Transformer Architecture to computer vision.

Here are some of my personal favourite resources on Transformers:
1. [The illustrated Transformer](http://jalammar.github.io/illustrated-transformer/) by Jay Alammar
2. [The Annotated Transformer](https://nlp.seas.harvard.edu/2018/04/03/attention.html) by Harvard NLP 
3. [Introduction to the Transformer](https://www.youtube.com/watch?v=AFkGPmU16QA&list=PLtmWHNX-gukKocXQOkQjuVxglSDYWsSh9&index=18&t=0s) by Rachel Thomas and Jeremy Howard

Though a biased recommendation, I would also like to recommend the reader to my previous post [The Annotated GPT-2](https://amaarora.github.io/2020/02/18/annotatedGPT2.html).

## Introduction
At the time of release, the [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929) has received quite a bit of "attention" from the community. This was the first paper to get some astonishing results on the ImageNet dataset using the Transformer architecture. While there have been attempts made in the past to apply Transformers in the context of image processing ([1](https://arxiv.org/abs/1802.05751), [2](https://arxiv.org/abs/1906.05909), [3](https://arxiv.org/abs/2003.07853)), the Vision Transformer was one of the first to apply Transformers to full-sizes images. 

**NOTE:** I will be using Vision Transforms, ViT interchangeably throughout this blog post and both refer to [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929) paper.

The key contributions from this paper were not in terms of a new architecture, but rather the application of an existing architecture (Transformers), to a new field. It is the **training method** and the **dataset used to pretrain the network**, that were key for ViT to get excellent results compared to SOTA (State of the Art) on ImageNet.

So, there isn't a lot of new things to introduce in this post, but rather how to use the existing knowledge and apply the Transformer architecutre to images. Thus, if the reader knows about Transformers, this blog post and the research paper itself should be a fairly simple read.

## Method
So, let's get right into the method and understand all the magic in the training pipeline. 

