{
 "cells": [
  {
   "cell_type": "raw",
   "id": "fb8e9db2",
   "metadata": {},
   "source": [
    "---\n",
    "title: The Annotated CLIP\n",
    "subtitle: Learning Transferable Visual Models From Natural Language Supervision\n",
    "description: | \n",
    "    This post presents an annotated version of the CLIP paper in the form of a line-by-line implementation in PyTorch. This document itself is a working notebook, and should be a completely usable implementation. All code has been copied from the official CLIP implementation.\n",
    "categories:\n",
    "  - Multimodal\n",
    "  - Transformers\n",
    "author: Aman Arora\n",
    "date: \"03/03/2023\"\n",
    "toc: true\n",
    "number-sections: true\n",
    "title-block-banner: true\n",
    "bibliography: ../references.bib\n",
    "reference-location: margin\n",
    "citation-location: margin\n",
    "code-fold: false\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4cccfdc",
   "metadata": {},
   "source": [
    "## Personal Updates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf996854",
   "metadata": {},
   "source": [
    "Hello, and welcome back everybody to the blog! This is my first blog of the year 2023 and as publicly announced on [Twitter](https://twitter.com/amaarora/status/1623082761052635136), I am returning to blogging with a commitment of 1 blog a week, planned to be released every Monday at 9am AEST. "
   ]
  },
  {
   "cell_type": "raw",
   "id": "8d2d1fd4",
   "metadata": {},
   "source": [
    "<blockquote class=\"twitter-tweet tw-align-center\"><p lang=\"en\" dir=\"ltr\">Starting 01 Mar, 2023 I’ll be going back to blogging 1 post a week every Monday at 9am AEST. <br><br>These blogs will be about AI research, new technologies, updates, frameworks, Kaggle competitions and more. <br><br>If you have a topic that you’d like me to cover, please let me know. :)</p>&mdash; Aman Arora (@amaarora) <a href=\"https://twitter.com/amaarora/status/1623082761052635136?ref_src=twsrc%5Etfw\">February 7, 2023</a></blockquote> <script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0fc45bf",
   "metadata": {},
   "source": [
    "Also, in case you missed it, I was also recently interviewed by [Radek Osmulski](https://twitter.com/radekosmulski) - **\"How to Blog to Advance Your Career and Learn Faster\"** (in AI). In the video, we discuss and talk about my motivation for writing blogs, blogging to advance your career and learn, how to get started with blogging & more!"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6e7c5b73",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\"><iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/1nMckFzGcd8\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen></iframe></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee66138",
   "metadata": {},
   "source": [
    "I have also updated my personal blog to use [Quarto](https://quarto.org/). The idea is to release all future blog posts which are working Jupyter Notebooks themeselves."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bcb1ad9",
   "metadata": {},
   "source": [
    "Now, with personal updates out of the way, let's get started with CLIP. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a319e0a",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f40040e",
   "metadata": {},
   "source": [
    "So what is CLIP and what are we going to cover in this blog post?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da65b61",
   "metadata": {},
   "source": [
    "In this blog post we will be going through the CLIP research paper - *\"Learning Transferable Visual Models From Natural Language Supervision\"* (@clip), and also look at the model implementation in PyTorch. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af150472",
   "metadata": {},
   "source": [
    "From the official blog [\"CLIP: Connecting text and images\"](https://openai.com/research/clip), \n",
    "\n",
    "> We’re introducing a neural network called CLIP which efficiently learns visual concepts from natural language supervision. CLIP can be applied to any visual classification benchmark by simply providing the names of the visual categories to be recognized, similar to the “zero-shot” capabilities of GPT-2 and GPT-3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d10d786",
   "metadata": {},
   "source": [
    "In my head, if I am to summarise **\"What is CLIP?\"**, the answer would be: \n",
    "\n",
    "\"CLIP\" is a way to effective pretrain vision models (& more), which is architecture agnostic, by training on huge amounts of data much beyond ImageNet. In CLIP, we are not learning from expensive labels, but rather, model learns visual representations from free text. \n",
    "\n",
    "Learning good visual and vision-language representations is critical to solving computer vision problems — image retrieval, image classification, video understanding."
   ]
  },
  {
   "cell_type": "raw",
   "id": "3f56bb9c",
   "metadata": {},
   "source": [
    "As part of this paper the authors introduce the CLIP \"model\", which is trained on 400M (image, text) pairs. It is a neural network that learns visual concepts from natural language supervision. But, in no way is CLIP limited to the specific model architectures introduced in the paper.\n",
    "\n",
    "\n",
    "::: {.callout-note}\n",
    "**CLIP Is architecture agnostic.** You can use any models as visual and text encoders.\n",
    "\n",
    "A team led by [Ross Wightman](https://rwightman.com/), [Cade Gordon](http://cadegordon.io/), and [Vaishaal Shankar](http://vaishaal.com/) have an open source repository [OpenCLIP](https://github.com/mlfoundations/open_clip) (@openclip) that is an open source implementation of CLIP and enables training  any vision models with contrastive image-text supervision. Recently, Ross Wightman [announced](https://twitter.com/wightmanr/status/1630300201075494912) an 847M param ConvNext model trained via CLIP training that achieves 79.43% ImageNet zero-shot eval.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1091abb0",
   "metadata": {},
   "source": [
    "## Motivation for CLIP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20d0c69",
   "metadata": {},
   "source": [
    "Why was CLIP needed? What problem did it solve?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58194cf9",
   "metadata": {},
   "source": [
    "Please note that CLIP was written in 2021, at a time where text transformer based models like GPT-3 were then competitive across many tasks with bespoke models on various benchmark datasets, swhile requiring little to no dataset specific training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e164089d",
   "metadata": {},
   "source": [
    "From the paper: \n",
    "    \n",
    "*Task-agnostic objectives such as autoregressive and masked language modeling have scaled across many orders of magnitude in compute, model capacity, and data, steadily improving capabilities. The development of “text-to-text” as a standardized input-output interface (McCann et al., 2018; Radford et al., 2019; Raffel et al., 2019) has enabled taskagnostic architectures to zero-shot transfer to downstream datasets removing the need for specialized output heads or dataset specific customization. Flagship systems like GPT-3 (Brown et al., 2020) are now competitive across many tasks with bespoke models while requiring little to no dataset specific training data.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb981810",
   "metadata": {},
   "source": [
    "But, for vision based models, it was still standard practice to pre-train models on crowd-labeled datasets such as ImageNet. The question then is ***Could scalable pre-training methods which learn directly from web text result in a similar breakthrough in computer vision?***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c434a7e",
   "metadata": {},
   "source": [
    "Before CLIP, there had been few research papers - VirTex (Desai & Johnson, 2020), ICMLM (Bulent Sariyildiz et al., 2020), and ConVIRT (Zhang et al., 2020) that demonstrated the potential of transformer-based language modeling, masked language modeling, and contrastive objectives to learn image representations from text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16abc86f",
   "metadata": {},
   "source": [
    "Therefore, this idea of learning visual representations from text in CLIP isn't new at all, but, the zero shot performane on ImageNet before CLIP was around 15%, much lower than 88.4% accuracy of state of the art at the time. (Xie et al., 2020)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623adff3",
   "metadata": {},
   "source": [
    "Kolesnikov et al. (2019) and Dosovitskiy et al. (2020) also demonstrated large gains on a broader set of transfer benchmarks by pre-training models to predict the classes of the noisily labeled JFT-300M dataset. However, both works carefully design, and in the process limit, their supervision to 1000 and 18291 classes respectively.\n",
    "\n",
    "*Natural language is able to express, and therefore supervise, a much wider set of visual concepts through its generality. Both approaches also use static softmax classifiers to perform prediction and lack a mechanism for dynamic outputs. This severely curtails their flexibility and limits their “zero-shot” capabilities.*"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0c2ac66f",
   "metadata": {},
   "source": [
    "::: {.callout-note}\n",
    "CLIP was the first paper to demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet (more on the dataset later). CLIP researchers were able to match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0cdafdc",
   "metadata": {},
   "source": [
    "As I am catching up with deep learning research, one latest update that I've seen happen in the last year or so is the shear scale of models. Pretraining on huge amounts of data at scale has led to further advancements in the field of deep learning. \n",
    "\n",
    "Also, CLIP image and text encoders are a critical part of [Stable Diffusion](https://stability.ai/blog/stable-diffusion-public-release), released last year. "
   ]
  },
  {
   "cell_type": "raw",
   "id": "3daad039",
   "metadata": {},
   "source": [
    "::: {.callout-note}\n",
    "CLIP isn't the only model that learns visual features from natural language supervision. Google also released a model architecture called Align (@align) that pretrains text and image encoders using data at scale. \n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82511edd",
   "metadata": {},
   "source": [
    "*We find that CLIP, similar to the GPT family, learns to perform a wide set of tasks during pre-training including OCR, geo-localization, action recognition, and many others. We measure this by benchmarking the zero-shot transfer performance of CLIP on over 30 existing datasets and find it can be competitive with prior task-specific supervised models. We also confirm these findings with linear-probe representation learning analysis and show that CLIP outperforms the best publicly available ImageNet model while also being more computationally efficient. We additionally find that zero-shot CLIP models are much more robust than equivalent accuracy supervised ImageNet models which suggests that zero-shot evaluation of task-agnostic models is much more representative of a model’s capability.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1220b7a8",
   "metadata": {},
   "source": [
    "## Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a8c38e",
   "metadata": {},
   "source": [
    "*At the core of our approach is the idea of learning perception from supervision contained in natural language. As discussed in the introduction, this is not at all a new idea, however terminology used to describe work in this space is varied, even seemingly contradictory, and stated motivations are diverse. Zhang et al. (2020), Gomez et al. (2017), Joulin et al. (2016), and Desai & Johnson (2020) all introduce methods which learn visual representations from text paired with images but describe their approaches as unsupervised, self-supervised*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430b7ebc",
   "metadata": {},
   "source": [
    "*Learning from natural language has several potential strengths over other training methods.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0936247",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859bfe8f",
   "metadata": {},
   "source": [
    "In this section I will present the summary of CLIP architecture from the paper. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e8d70b",
   "metadata": {},
   "source": [
    "![Summary of CLIP approach](../images/clip.png){#fig-clip}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4583610",
   "metadata": {},
   "source": [
    "From the paper: \n",
    "\n",
    "*Given a batch of $N$ (image, text) pairs, CLIP is trained to predict which of the $N × N$ possible (image, text) pairings across a batch actually occurred. To do this, CLIP learns a multi-modal embedding space by jointly training an image encoder and text encoder to maximize the cosine similarity of the image and text embeddings of the $N$ real pairs in the batch while minimizing the cosine similarity of the embeddings of the $N^2 − N$ incorrect pairings. We optimize a symmetric cross entropy loss over these similarity scores.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225c43e2",
   "metadata": {},
   "source": [
    "If the above doesn't make complete sense, that's okay. Let's look at the Pseudo Code from the paper. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23332e0",
   "metadata": {},
   "source": [
    "```\n",
    "# image_encoder - ResNet or Vision Transformer \n",
    "# text_encoder - CBOW or Text Transformer \n",
    "# I[n, h, w, c] - minibatch of aligned images \n",
    "# T[n, l] - minibatch of aligned texts \n",
    "# W_i[d_i, d_e] - learned proj of image to embed \n",
    "# W_t[d_t, d_e] - learned proj of text to embed \n",
    "# t - learned temperature parameter \n",
    "\n",
    "# extract feature representations of each modality \n",
    "I_f = image_encoder(I) #[n, d_i] \n",
    "T_f = text_encoder(T) #[n, d_t] \n",
    "\n",
    "# joint multimodal embedding [n, d_e] \n",
    "I_e = l2_normalize(np.dot(I_f, W_i), axis=1) \n",
    "T_e = l2_normalize(np.dot(T_f, W_t), axis=1) \n",
    "\n",
    "# scaled pairwise cosine similarities [n, n] \n",
    "logits = np.dot(I_e, T_e.T) * np.exp(t) \n",
    "\n",
    "# symmetric loss function \n",
    "labels = np.arange(n) \n",
    "loss_i = cross_entropy_loss(logits, labels, axis=0) \n",
    "loss_t = cross_entropy_loss(logits, labels, axis=1) \n",
    "loss = (loss_i + loss_t)/2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029dc446",
   "metadata": {},
   "source": [
    "Let's look at what it all means with the help of Microsoft Excel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4e110a",
   "metadata": {},
   "source": [
    "![Contrastive Loss](../images/contrastive_loss.png){#fig-contrastive-loss}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f89669",
   "metadata": {},
   "source": [
    "Let's just say we have 4 images - image of earrings,image of tea cup & saucer, image of furniture and an image of cake. \n",
    "\n",
    "Also, let's say we have 4 captions with each of the images - cute earrings, tea cup & saucer, dining furniture, orchid wedding cake. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea3fc7f",
   "metadata": {},
   "source": [
    "Essentially, with Contrastive Loss, as shown in @fig-contrastive-loss, what we want to do, is that we want the green diagonal to have high values and everywhere else to have lower values. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5a90d9",
   "metadata": {},
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2d0257",
   "metadata": {},
   "source": [
    "Having looked at the CLIP in theory, it is now time to look at the CLIP model architectures in code.\n",
    "\n",
    "We will first look at the Image Encoder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3480b7c",
   "metadata": {},
   "source": [
    "### Image Encoder "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd7a8f5",
   "metadata": {},
   "source": [
    "From section 2.4 of the CLIP paper: \n",
    "\n",
    "*We consider two different architectures for the image encoder. For the first, we use ResNet-50 (@resnet) as the base architecture for the image encoder due to its widespread adoption and proven performance. We make several modifications to the original version using the ResNetD improvements from @bag_of_tricks and the antialiased rect-2 blur pooling from Zhang (2019). We also replace the global average pooling layer with an attention pooling mechanism. The attention pooling is implemented as a single layer of “transformer-style” multi-head QKV attention where the query is conditioned on the global average-pooled representation of the image. For the second architecture, we experiment with the recently introduced Vision Transformer (ViT) (Dosovitskiy et al., 2020). We closely follow their implementation with only the minor modification of adding an additional layer normalization to the combined patch and position embeddings before the transformer and use a slightly different initialization scheme.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847f4b7f",
   "metadata": {},
   "source": [
    "So the first change from original ResNet to the ResNet architecture used in CLIP Image Encoder, is the stem. In the original ResNet architecture:\n",
    "\n",
    "*The input stem has a 7 × 7 convolution with an output channel of 64 and a stride of 2, followed by a 3 × 3 max pooling layer also with a stride of 2. The input stem reduces the input width and height by 4 times and increases its channel size to 64.*\n",
    "\n",
    "```python\n",
    "self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "self.bn1 = norm_layer(self.inplanes)\n",
    "self.relu = nn.ReLU(inplace=True)\n",
    "self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57528a5",
   "metadata": {},
   "source": [
    "*A 7 × 7 convolution is 5.4 times more expensive than a 3 × 3 convolution. So this tweak replacing the 7 × 7 convolution in the input stem with three conservative 3 × 3 convolutions, with the first and second convolutions have\n",
    "their output channel of 32 and a stride of 2, while the last convolution uses a 64 output channel.*\n",
    "```python \n",
    "# the 3-layer stem\n",
    "self.conv1 = nn.Conv2d(\n",
    "    3, width // 2, kernel_size=3, stride=2, padding=1, bias=False\n",
    ")\n",
    "self.bn1 = nn.BatchNorm2d(width // 2)\n",
    "self.relu1 = nn.ReLU(inplace=True)\n",
    "self.conv2 = nn.Conv2d(\n",
    "    width // 2, width // 2, kernel_size=3, padding=1, bias=False\n",
    ")\n",
    "self.bn2 = nn.BatchNorm2d(width // 2)\n",
    "self.relu2 = nn.ReLU(inplace=True)\n",
    "self.conv3 = nn.Conv2d(width // 2, width, kernel_size=3, padding=1, bias=False)\n",
    "self.bn3 = nn.BatchNorm2d(width)\n",
    "self.relu3 = nn.ReLU(inplace=True)\n",
    "self.avgpool = nn.AvgPool2d(2)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
